---
systemd:
  units:
    - name: etcd-member.service
      enable: true
      dropins:
        - name: 40-etcd-cluster.conf
          contents: |
            [Service]
            Environment="ETCD_IMAGE_TAG=v3.3.9"
            Environment="ETCD_NAME=${etcd_name}"
            Environment="ETCD_ADVERTISE_CLIENT_URLS=https://${etcd_domain}:2379"
            Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=https://${etcd_domain}:2380"
            Environment="ETCD_LISTEN_CLIENT_URLS=https://0.0.0.0:2379"
            Environment="ETCD_LISTEN_PEER_URLS=https://0.0.0.0:2380"
            Environment="ETCD_LISTEN_METRICS_URLS=http://0.0.0.0:2381"
            Environment="ETCD_INITIAL_CLUSTER=${etcd_initial_cluster}"
            Environment="ETCD_STRICT_RECONFIG_CHECK=true"
            Environment="ETCD_SSL_DIR=/etc/ssl/etcd"
            Environment="ETCD_TRUSTED_CA_FILE=/etc/ssl/certs/etcd/server-ca.crt"
            Environment="ETCD_CERT_FILE=/etc/ssl/certs/etcd/server.crt"
            Environment="ETCD_KEY_FILE=/etc/ssl/certs/etcd/server.key"
            Environment="ETCD_CLIENT_CERT_AUTH=true"
            Environment="ETCD_PEER_TRUSTED_CA_FILE=/etc/ssl/certs/etcd/peer-ca.crt"
            Environment="ETCD_PEER_CERT_FILE=/etc/ssl/certs/etcd/peer.crt"
            Environment="ETCD_PEER_KEY_FILE=/etc/ssl/certs/etcd/peer.key"
            Environment="ETCD_PEER_CLIENT_CERT_AUTH=true"
    - name: docker-tcp.socket
      enable: true
      contents: |
        [Unit]
        Description=Docker Socket for the API
        [Socket]
        ListenStream=2375
        Service=docker.service
        BindIPv6Only=both
        [Install]
        WantedBy=sockets.target
    - name: docker.service
      enable: true
    - name: locksmithd.service
      mask: true
    - name: wait-for-dns.service
      enable: true
      contents: |
        [Unit]
        Description=Wait for DNS entries
        Wants=systemd-resolved.service
        Before=kubelet.service
        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/bin/sh -c 'while ! /usr/bin/grep '^[^#[:space:]]' /etc/resolv.conf > /dev/null; do sleep 1; done'
        [Install]
        RequiredBy=kubelet.service
        RequiredBy=etcd-member.service
    - name: kubelet.service
      enable: true
      contents: |
        [Unit]
        Description=Kubelet via Hyperkube
        Wants=rpc-statd.service
        [Service]
        EnvironmentFile=/etc/kubernetes/kubelet.env
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/cache/kubelet-pod.uuid \
          --volume=resolv,kind=host,source=/etc/resolv.conf \
          --mount volume=resolv,target=/etc/resolv.conf \
          --volume var-lib-cni,kind=host,source=/var/lib/cni \
          --mount volume=var-lib-cni,target=/var/lib/cni \
          --volume var-lib-calico,kind=host,source=/var/lib/calico \
          --mount volume=var-lib-calico,target=/var/lib/calico \
          --volume opt-cni-bin,kind=host,source=/opt/cni/bin \
          --mount volume=opt-cni-bin,target=/opt/cni/bin \
          --volume var-log,kind=host,source=/var/log \
          --mount volume=var-log,target=/var/log \
          --hosts-entry=host \
          --insecure-options=image"
        ExecStartPre=/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/bin/mkdir -p /etc/kubernetes/cni/net.d
        ExecStartPre=/bin/mkdir -p /etc/kubernetes/checkpoint-secrets
        ExecStartPre=/bin/mkdir -p /etc/kubernetes/inactive-manifests
        ExecStartPre=/bin/mkdir -p /var/lib/cni
        ExecStartPre=/bin/mkdir -p /var/lib/calico
        ExecStartPre=/bin/mkdir -p /var/lib/kubelet/volumeplugins
        ExecStartPre=/usr/bin/bash -c "grep 'certificate-authority-data' /etc/kubernetes/kubeconfig | awk '{print $2}' | base64 -d > /etc/kubernetes/ca.crt"
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/cache/kubelet-pod.uuid
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --anonymous-auth=false \
          --authentication-token-webhook \
          --authorization-mode=Webhook \
          --client-ca-file=/etc/kubernetes/ca.crt \
          --cluster_dns=${k8s_dns_service_ip} \
          --cluster_domain=${cluster_domain_suffix} \
          --cni-conf-dir=/etc/kubernetes/cni/net.d \
          --exit-on-lock-contention \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          --lock-file=/var/run/lock/kubelet.lock \
          --network-plugin=cni \
          --node-labels=node-role.kubernetes.io/master \
          --node-labels=node-role.kubernetes.io/controller="true" \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --register-with-taints=node-role.kubernetes.io/master=:NoSchedule \
          --volume-plugin-dir=/var/lib/kubelet/volumeplugins
          --cloud-provider =${cloud_provider}
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/cache/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
    - name: bootkube.service
      contents: |
        [Unit]
        Description=Bootstrap a Kubernetes cluster
        ConditionPathExists=!/opt/bootkube/init_bootkube.done
        [Service]
        Type=oneshot
        RemainAfterExit=true
        WorkingDirectory=/opt/bootkube
        ExecStart=/opt/bootkube/bootkube-start
        ExecStartPost=/bin/touch /opt/bootkube/init_bootkube.done
        [Install]
        WantedBy=multi-user.target
storage:
  directories:
    - path: /etc/docker
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0755
    - path: /opt/bin
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0755
    - path: /etc/sysctl.d
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0755
    - path: /etc/motd.d
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0755
  files:
    - path: /etc/coreos/update.conf
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0644
      contents:
        inline: |
          REBOOT_STRATEGY "etcd-lock"
    - path: /opt/bin/docker-enter
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0644
      contents:
        inline: |
          #!/bin/bash
          all_containers="$(docker ps | grep -oE '[^ ]+$' | grep -v NAMES)"
          if [[ $# -ne 1 ]] ; then
            echo "ERROR: wrong amount of arguments"
            echo "USAGE: $0 <container-name>"
            echo "List of Currently Running Containers"
            echo $${all_containers} | tr ' ' '\n'
            exit 1
          fi
          container="$1"
          matches="$(echo $${all_containers} | tr ' ' '\n' | grep $${container} | wc -l)"
          if [[ $${matches} -gt 1 ]] ; then
            echo "ERROR: too many matches"
            echo $${all_containers} | tr ' ' '\n' | grep $${container}
            exit 1
          elif [[ $${matches} -lt 1 ]] ; then
            echo "ERROR: not enough matches"
            echo "List of Currently Running Containers"
            echo $${all_containers} | tr ' ' '\n'
            exit 1
          else
            container="$(echo $${all_containers} | tr ' ' '\n' | grep $${container})"
            echo "ENTERING CONTAINER: $${container}"
            docker exec -ti $${container} bash -l
            exit 0
          fi
    - path: /root/.dockercfg
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0644
      contents:
        inline: |
          {
            "https://index.docker.io/v1/": {
              "auth": "${dockerhub_key}",
              "email": ""
            },
            "quay.io": {
              "auth": "${quay_key}"
              "email": ""
            }
          }
    - path: /etc/motd.d/default.conf
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0644
      contents:
        inline: |
          "

          This system is made available to authorized Employees only.
          Unauthorized access, attempts to defeat or circumvent security features,
          to use the system for other than intended purposes, to deny service to
          authorized users, or otherwise to interfere with the system or its operation
          is strictly prohibited. Evidence of such acts will be disclosed to law
          enforcement authorities and may result in criminal prosecution.
          If you are not an authorized employee please logout immediately.

          "
    - path: /etc/modules-load.d/nf.conf
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0644
      contents:
        inline: |
          nf_conntrack
    - path: /etc/sysctl.d/startup.conf
      filesystem: root
      user:
        name: root
      group:
        name: root
      mode: 0644
      contents:
        inline: |
          net.ipv4.conf.all.route_localnet=1
          fs.file-max=2097152

          # for es
          vm.max_map_count=262144
          # Allow for more PIDs
          kernel.pid_max=65535

          # Restrict core dumps
          fs.suid_dumpable=0

          # Hide exposed kernel pointers
          kernel.kptr_restrict=1

          # Do less swapping
          vm.swappiness=30
          vm.dirty_ratio=30
          vm.dirty_background_ratio=5

          # 50% overcommitment of available memory
          vm.overcommit_ratio=50
          vm.overcommit_memory=0

          # Keep at least 64MB of free RAM space available
          vm.min_free_kbytes=65535


          #Prevent SYN attack
          net.ipv4.tcp_syncookies=1
          net.ipv4.tcp_syn_retries=2
          net.ipv4.tcp_synack_retries=2
          net.ipv4.tcp_max_syn_backlog=4096

          # Enable IP spoofing protection, turn on source route verification
          net.ipv4.conf.all.rp_filter=1
          net.ipv4.conf.default.rp_filter=1

          # Disable ICMP Redirect Acceptance
          net.ipv4.conf.all.accept_redirects=0
          net.ipv4.conf.default.accept_redirects=0
          net.ipv4.conf.all.secure_redirects=0
          net.ipv4.conf.default.secure_redirects=0
          net.ipv6.conf.all.accept_redirects=0
          net.ipv6.conf.default.accept_redirects=0

          # Enable Log Spoofed Packets, Source Routed Packets, Redirect Packets
          net.ipv4.conf.all.log_martians=1
          net.ipv4.conf.default.log_martians=1

          # Decrease the time default value for tcp_fin_timeout connection
          net.ipv4.tcp_fin_timeout=7

          # Decrease the time default value for connections to keep alive
          net.ipv4.tcp_keepalive_time=300
          net.ipv4.tcp_keepalive_probes=5
          net.ipv4.tcp_keepalive_intvl=15

          # Don't relay bootp
          net.ipv4.conf.all.bootp_relay=0

          # Don't proxy arp for anyone
          net.ipv4.conf.all.proxy_arp=0

          # Turn on the tcp_timestamps, accurate timestamp make TCP congestion control algorithms work better
          net.ipv4.tcp_timestamps=1

          # Don't ignore directed pings
          net.ipv4.icmp_echo_ignore_all=0

          # Enable ignoring broadcasts request
          net.ipv4.icmp_echo_ignore_broadcasts=1

          # Enable bad error message Protection
          net.ipv4.icmp_ignore_bogus_error_responses=1

          # Enable a fix for RFC1337 - time-wait assassination hazards in TCP
          net.ipv4.tcp_rfc1337=1

          # For servers with tcp-heavy workloads, enable 'fq' queue management scheduler (kernel > 3.12)
          net.core.default_qdisc=fq

          # Turn on the tcp_window_scaling
          net.ipv4.tcp_window_scaling=1

          # Increase the read-buffer space allocatable
          net.ipv4.tcp_rmem=8192 87380 16777216
          net.ipv4.udp_rmem_min=16384
          net.core.rmem_default=262144
          net.core.rmem_max=16777216

          # Increase the write-buffer-space allocatable
          net.ipv4.tcp_wmem=8192 65536 16777216
          net.ipv4.udp_wmem_min=16384
          net.core.wmem_default=262144
          net.core.wmem_max=16777216

          # Increase number of incoming connections
          net.core.somaxconn=32768

          # Increase number of incoming connections backlog
          net.core.netdev_max_backlog=16384
          net.core.dev_weight=64

          # Increase the maximum amount of option memory buffers
          net.core.optmem_max=65535

          # Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks
          net.ipv4.tcp_max_tw_buckets=1440000

          # try to reuse time-wait connections, but don't recycle them (recycle can break clients behind NAT)
          net.ipv4.tcp_tw_recycle=0
          net.ipv4.tcp_tw_reuse=1

          # Limit number of orphans, each orphan can eat up to 16M (max wmem) of unswappable memory
          net.ipv4.tcp_max_orphans=16384
          net.ipv4.tcp_orphan_retries=0

          # don't cache ssthresh from previous connection
          net.ipv4.tcp_no_metrics_save=1
          net.ipv4.tcp_moderate_rcvbuf=1

          # Increase size of RPC datagram queue length
          net.unix.max_dgram_qlen=50

          # Don't allow the arp table to become bigger than this
          net.ipv4.neigh.default.gc_thresh3=2048

          # Tell the gc when to become aggressive with arp table cleaning.
          # Adjust this based on size of the LAN. 1024 is suitable for most /24 networks
          net.ipv4.neigh.default.gc_thresh2=1024

          # Adjust where the gc will leave arp table alone - set to 32.
          net.ipv4.neigh.default.gc_thresh1=32

          # Adjust to arp table gc to clean-up more often
          net.ipv4.neigh.default.gc_interval=30

          # Increase TCP queue length
          net.ipv4.neigh.default.proxy_qlen=96
          net.ipv4.neigh.default.unres_qlen=6

          # Enable Explicit Congestion Notification (RFC 3168), disable it if it doesn't work for you
          net.ipv4.tcp_ecn=1
          net.ipv4.tcp_reordering=3

          # How many times to retry killing an alive TCP connection
          net.ipv4.tcp_retries2=15
          net.ipv4.tcp_retries1=3

          # Avoid falling back to slow start after a connection goes idle
          # keeps our cwnd large with the keep alive connections (kernel > 3.6)
          net.ipv4.tcp_slow_start_after_idle=0

          # Allow the TCP fastopen flag to be used, beware some firewalls do not like TFO! (kernel > 3.7)
          net.ipv4.tcp_fastopen=3

          # This will enusre that immediatly subsequent connections use the new values
          net.ipv4.route.flush=1
          net.ipv6.route.flush=1
    - path: /etc/kubernetes/kubeconfig
      filesystem: root
      mode: 0644
      contents:
        inline: |
          ${kubeconfig}
    - path: /etc/kubernetes/kubelet.env
      filesystem: root
      mode: 0644
      contents:
        inline: |
          KUBELET_IMAGE_URL=docker://k8s.gcr.io/hyperkube
          KUBELET_IMAGE_TAG=v1.11.2
    - path: /etc/sysctl.d/max-user-watches.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          fs.inotify.max_user_watches=16184
    - path: /opt/bootkube/bootkube-start
      filesystem: root
      mode: 0544
      user:
        id: 500
      group:
        id: 500
      contents:
        inline: |
          #!/bin/bash
          # Wrapper for bootkube start
          set -e
          # Move experimental manifests
          [ -n "$(ls /opt/bootkube/assets/manifests-*/* 2>/dev/null)" ] && mv /opt/bootkube/assets/manifests-*/* /opt/bootkube/assets/manifests && rm -rf /opt/bootkube/assets/manifests-*
          BOOTKUBE_ACI="$${BOOTKUBE_ACI:-quay.io/coreos/bootkube}"
          BOOTKUBE_VERSION="$${BOOTKUBE_VERSION:-v0.13.0}"
          BOOTKUBE_ASSETS="$${BOOTKUBE_ASSETS:-/opt/bootkube/assets}"
          exec /usr/bin/rkt run \
            --trust-keys-from-https \
            --volume assets,kind=host,source=$${BOOTKUBE_ASSETS} \
            --mount volume=assets,target=/assets \
            --volume bootstrap,kind=host,source=/etc/kubernetes \
            --mount volume=bootstrap,target=/etc/kubernetes \
            $${RKT_OPTS} \
            $${BOOTKUBE_ACI}:$${BOOTKUBE_VERSION} \
            --net=host \
            --dns=host \
            --exec=/bootkube -- start --asset-dir=/assets "$@"
passwd:
  users:
    - name: core
      ssh_authorized_keys:
        - "${ssh_authorized_key}"
